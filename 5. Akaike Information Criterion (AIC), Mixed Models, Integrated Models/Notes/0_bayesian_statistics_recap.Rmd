---
title: "Bayesian statistics recap"
output: html_notebook
---

In this notebook we are reviewing the most important concepts on Bayesian statistics. Note this is not part of any weeks of the course,
but I wanted to refresh some ideas for myself.

## Notation

First, let's define some notation:

- We define $x$ as the sample data of the population $X$.
- We define $\theta$ as a set parameter of the underlying data generation mechanism.

## Summary of key concepts

### Prior probability

Probability distribution that expresses knowledge about a random variable before any evidence is presented. The prior knowledge
includes the assumption of the underlying probability distribution $p$ and its parameters $\theta$ (e.g. $\theta$ could be the mean
of a normal distribution).

Note that the parameters of the distribution (i.e. hyperparameters) may also have hyperprior parameters,
which is known as hierarchical Bayes model (e.g. we could assume $\theta$ follows an additional distribution $p'$).

### Posterior probability

It represents the conditional probability of a random variable after some evidence is taken into account.

Given we have:

- A prior $p(\theta)$.
- A likelihood function $p(X=x|\theta)$.

The posterior is defined as:

$$
p(\theta | x)
$$

Note that the posterior can be computed in terms of the likelihood by using the Bayes theorem:

$$
p(\theta|x) = \frac{p(x | \theta) p(\theta)}{p(x)} \propto likelihood \times prior
$$

Note that $p(x)$ is the sum over all possible states of $\theta$:

$$
p(x) = \sum_i p(x|\theta_i)p(\theta_i)
$$

That becomes an integral in the continuous domain:

$$
p(x) = \int^b_a p(x|\theta)f(\theta)
$$

### Likelihood function

The likelihood function can be expressed as:

$$p(X=x|\theta), \ \mathcal{L(\theta|x)}, \ p_{\theta}(X=x)$$

Where $p$ is the probability mass function. It expresses the probability of generating the sample $x$ of the random variable $X$ as a function of $\theta$.

The likelihood function expresses a hypersurface over the hyperparameter space where each point can be interpreted as the probability of the data sample being generated by
a specific setting of parameters. Therefore, the highest peak in the likelihood function is called the maximum likelihood estimate.


### Maximum a posteriori (MAP)

It is the estimate that maximizes the posterior probability:

$$
MAP: \ \theta* = \underset{\theta}{\mathrm{argmax}} \ p(\theta|x) = \underset{\theta}{\mathrm{argmax}} \ p(x|\theta) p(\theta)

$$

### Maximum likelihood estimate (MLE)

It is a special type of $MLE$ assumes prior is constant and can be defined as:

$$
MLE: \ \theta* = \underset{\theta}{\mathrm{argmax}} \ p(X=x|\theta) = \underset{\theta}{\mathrm{argmax}} \ \mathcal{L(\theta|x)}
$$

[This](https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/) is a good post on the differences between $MAP$ and $MLE$.



## Bibliography

---
nocite: | 
  @bishop, @barber, @link
---

---
references:
- id: bishop
  title: Pattern Recognition and Machine Learning
  author:
  - family: Bishop
    given: Christopher M
  publisher: Springer
  type: book
  issued:
    year: 2006

- id: barber
  title: Bayesian Reasoning and Machine Learning
  author:
  - family: Barber
    given: David
  publisher: Cambridge University Press
  type: book
  issued:
    year: 2012
---
